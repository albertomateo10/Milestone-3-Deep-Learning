{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a119a-f96e-4baf-b397-fc6f4d27f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8d2a7-92f9-48a2-b6f3-5fa12ffdc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics[detection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ef0024-6161-43ca-a7e8-f0f2d01c5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Union, Any\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from torchvision.models import detection\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "813ba83b-f4b6-40f3-9688-ec2681976673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Define a seed for reproducibility. It allows experiment repetition obtaining the exact same results.\n",
    "    :param seed: integer number indicating which seed you want to use.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    # random.seed(seed)  # Python random module.\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_inner_model(model: detection) -> Any:\n",
    "    \"\"\"\n",
    "    PyTorch provides a model wrapper to enable multiple GPUs. This function returns the inner model (without wrapper).\n",
    "    :param model: Torch model, with or without nn.DataParallel wrapper.\n",
    "    :return: if model is wrapped, it returns the inner model (model.module). Otherwise, it returns the input model.\n",
    "    \"\"\"\n",
    "    return model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "\n",
    "def torch_load_cpu(load_path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Load the data saved from a trained model (model weights, optimizer state, last epoch number to resume training...)\n",
    "    :param load_path: string indicating the path to the data saved from a trained model.\n",
    "    :return: dictionary containing data saved from a trained model.\n",
    "    \"\"\"\n",
    "    return torch.load(load_path, map_location=lambda storage, loc: storage)  # Load on CPU\n",
    "\n",
    "def load_model_path(path: str, model: detection, device: torch.device, optimizer: torch.optim = None) -> Tuple[Any, Any, int]:\n",
    "    \"\"\"\n",
    "    Load the trained weights of a model into the given model.\n",
    "    :param path: string indicating the path to the trained weights of a model.\n",
    "    :param model: the model where you want to load the weights.\n",
    "    :param device: whether gpu or cpu is being used.\n",
    "    :param optimizer: the optimizer initialized before loading the weights.\n",
    "    :return:\n",
    "        model: Torchvision model.\n",
    "        optimizer: Torch optimizer.\n",
    "        initial_epoch: first epoch number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model state\n",
    "    load_data = torch_load_cpu(path)\n",
    "    model_ = get_inner_model(model)\n",
    "    model_.load_state_dict({**model_.state_dict(), **load_data.get('model', {})})\n",
    "\n",
    "    # Load rng state\n",
    "    torch.set_rng_state(load_data['rng_state'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_rng_state_all(load_data['cuda_rng_state'])\n",
    "\n",
    "    # Load optimizer state\n",
    "    if 'optimizer' in load_data and optimizer is not None:\n",
    "        optimizer.load_state_dict(load_data['optimizer'])\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    # Get initial epoch\n",
    "    initial_epoch = load_data['initial_epoch']\n",
    "\n",
    "    return model, optimizer, initial_epoch\n",
    "def torchvision_model(model_name: str, pretrained: bool = False, num_classes: int = 2) -> Any:\n",
    "    \"\"\"\n",
    "    Return a model from a list of Torchvision models.\n",
    "    :param model_name: name of the Torchvision model that you want to load.\n",
    "    :param pretrained: whether pretrained weights are going to be loaded or not.\n",
    "    :param num_classes: number of classes. Minimum is 2: 0 = background, 1 = object.\n",
    "    :return:\n",
    "        model: Torchvision model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Torchvision models\n",
    "    model_dict = {\n",
    "        'faster_rcnn_v1': detection.fasterrcnn_resnet50_fpn,\n",
    "        'faster_rcnn_v2': detection.fasterrcnn_resnet50_fpn_v2,\n",
    "        'faster_rcnn_v3': detection.fasterrcnn_mobilenet_v3_large_fpn,\n",
    "        # 'faster_rcnn_v4': detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "        # 'fcos_v1': detection.fcos_resnet50_fpn,\n",
    "        'retinanet_v1': detection.retinanet_resnet50_fpn,\n",
    "        'retinanet_v2': detection.retinanet_resnet50_fpn_v2,\n",
    "        'ssd_v1': detection.ssd300_vgg16,\n",
    "        'ssd_v2': detection.ssdlite320_mobilenet_v3_large,\n",
    "    }\n",
    "\n",
    "    # Create model and load pretrained weights (if pretrained=True)\n",
    "    if model_name in model_dict:\n",
    "        model = model_dict[model_name](weights='COCO_V1' if pretrained else None)\n",
    "\n",
    "        # Modify the model's output layer for the number of classes in your dataset\n",
    "        if 'faster_rcnn' in model_name:\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        elif 'retinanet' in model_name:\n",
    "            in_features = model.head.classification_head.cls_logits.in_channels\n",
    "            num_anchors = model.head.classification_head.num_anchors\n",
    "            model.head.classification_head = detection.retinanet.RetinaNetClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'fcos' in model_name:\n",
    "            in_features = model.head.classification_head.cls_logits.in_channels\n",
    "            num_anchors = model.head.classification_head.num_anchors\n",
    "            model.head.classification_head = detection.fcos.FCOSClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'ssd_v1' in model_name:\n",
    "            in_features = [module.in_channels for module in model.head.classification_head.module_list]\n",
    "            num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "            model.head.classification_head = detection.ssd.SSDClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'ssd_v2' in model_name:\n",
    "            in_features = [module[0][0].in_channels for module in model.head.classification_head.module_list]\n",
    "            num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "            model.head.classification_head = detection.ssd.SSDClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "\n",
    "    # Error: Model not in list\n",
    "        else:\n",
    "            assert False, 'Model {} not in list. Indicate a Torchvision model from the list.'.format(model_name)\n",
    "    else:\n",
    "        assert False, 'Model {} not in list. Indicate a Torchvision model from the list.'.format(model_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model(model_name: str, model_path: str = '', num_classes: int = 2,\n",
    "              lr_data: list = None, pretrained: bool = False,\n",
    "              use_gpu: bool = False) -> Tuple[Any, Any, int, torch.device]:\n",
    "    \"\"\"\n",
    "    Main function to create and load the model.\n",
    "    :param model_name: name of the Torchvision model to load.\n",
    "    :param model_path: path to the model.\n",
    "    :param num_classes: number of classes. Minimum is 2: 0 = background, 1 = object.\n",
    "    :param lr_data: list containing [learning rate, learning rate momentum, learning rate decay].\n",
    "    :param pretrained: whether Torch pretrained weights on COCO dataset are going to be used or not.\n",
    "    :param use_gpu: whether to use GPU or CPU.\n",
    "    :return:\n",
    "        model: Torch model.\n",
    "        optimizer: Torch optimizer.\n",
    "        initial_epoch: first epoch number.\n",
    "        device: torch device indicating whether to use GPU or CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define device (GPU or CPU)\n",
    "    device_name = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device_name)\n",
    "\n",
    "    # Load Torchvision model\n",
    "    model = torchvision_model(model_name, pretrained, num_classes).to(device)\n",
    "    if use_gpu and torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Define the optimizer\n",
    "    if lr_data:\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        # optimizer = torch.optim.SGD(params, lr=lr_data[0], momentum=lr_data[1], weight_decay=lr_data[2])\n",
    "        optimizer = torch.optim.Adadelta(params, lr=lr_data[0], rho=lr_data[1], eps=lr_data[2])\n",
    "    else:\n",
    "        optimizer = None\n",
    "\n",
    "    # Load trained weights, optimizer state, and initial epoch\n",
    "    if os.path.isfile(model_path):\n",
    "        print('  [*] Loading Torch model from {}'.format(model_path))\n",
    "        model, optimizer, initial_epoch = load_model_path(model_path, model, device, optimizer)\n",
    "    else:\n",
    "        initial_epoch = 0\n",
    "        print('Weights not found')\n",
    "\n",
    "    return model, optimizer, initial_epoch, device\n",
    "\n",
    "def clip_grad_norms(param_groups, max_norm=np.inf) -> Tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Limit (clip) the norm of the gradients to avoid gradient explosion.\n",
    "    :param param_groups: parameters of the optimizer, from which gradients are extracted.\n",
    "    :param max_norm: maximum value for the norm of the gradient. max_norm = 0 avoids clipping.\n",
    "    :return:\n",
    "        grad_norms: gradients.\n",
    "        grad_norms_clipped: clipped gradients.\n",
    "    \"\"\"\n",
    "    # Get gradients\n",
    "    grad_norms = [\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            group['params'],\n",
    "            max_norm if max_norm > 0 else np.inf,  # Inf so no clipping but still call to calc\n",
    "            norm_type=2\n",
    "        )\n",
    "        for group in param_groups\n",
    "    ]\n",
    "\n",
    "    # Clip gradients\n",
    "    grad_norms_clipped = [min(g_norm, max_norm) for g_norm in grad_norms] if max_norm > 0 else grad_norms\n",
    "    return grad_norms, grad_norms_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1db07af-f4ae-4c51-9366-c4cea148625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data functions\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path_dataset: str, resize_shape: tuple = None, transform: transforms.Compose = None) -> None:\n",
    "        \"\"\"\n",
    "        Custom dataset that feeds the network during train, validation, and test.\n",
    "        :param path_dataset: path to the dataset.\n",
    "        :param resize_shape: tuple indicating height and width to resize images (for faster performance).\n",
    "        :param transform: list of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        self.annotations = parse_annotations(path_dataset)\n",
    "        self.resize_shape = resize_shape\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Get an index corresponding to one of the images and return the image and its annotation.\n",
    "        :param idx: index of image to load.\n",
    "        :return:\n",
    "            image: Torch tensor containing the image with shape (Channels, Height, Width).\n",
    "            targets: dictionary with the bounding boxes (boxes) and class labels (labels) of each annotated object.\n",
    "        \"\"\"\n",
    "        # Get one annotation for the current index\n",
    "        annotation = self.annotations[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(annotation['path_image']).convert(\"RGB\")\n",
    "\n",
    "        # Load bounding boxes\n",
    "        boxes = self.annotations[idx]['boxes']\n",
    "        boxes = torch.Tensor(boxes)\n",
    "\n",
    "        # Load labels (class of the object)\n",
    "        labels = self.annotations[idx]['labels']\n",
    "        labels = torch.Tensor(labels).type(torch.int64)  # Specify that labels are int\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            w, h = image.size\n",
    "            image, boxes, class_labels = self.transform(image, boxes, labels)\n",
    "            if self.resize_shape:\n",
    "                boxes = resize_boxes(boxes, self.resize_shape, (h, w))\n",
    "\n",
    "        # Torchvision models use this structure for boxes and labels\n",
    "        targets = {'boxes': boxes, 'labels': torch.Tensor(labels)}\n",
    "\n",
    "        return image, targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Length of the dataset.\n",
    "        :return: number of annotated images contained in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "def resize_boxes(boxes: torch.Tensor, resize_shape: tuple, image_shape: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Resize the shape of the bounding boxes when the size of the images is also resized.\n",
    "    :param boxes: Torch tensor containing bounding boxes with format: [x_min, y_min, x_max, y_max].\n",
    "    :param resize_shape: new image size.\n",
    "    :param image_shape: previous image size.\n",
    "    :return:\n",
    "        boxes: resized bounding boxes.\n",
    "    \"\"\"\n",
    "    boxes[:, 0] *= resize_shape[1] / image_shape[1]\n",
    "    boxes[:, 1] *= resize_shape[0] / image_shape[0]\n",
    "    boxes[:, 2] *= resize_shape[1] / image_shape[1]\n",
    "    boxes[:, 3] *= resize_shape[0] / image_shape[0]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_transform(norm: tuple, resize_shape: Union[tuple, None]) -> transforms.Compose:\n",
    "    \"\"\"\n",
    "    Define data transformations and apply them to the dataset.\n",
    "    :param norm: mean and std required by each Torchvision model to normalize the input images.\n",
    "    :param resize_shape: new image size.\n",
    "    :return:\n",
    "        transform: list of transforms to apply to the images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert images to Torch tensors and apply the previous normalization\n",
    "    t = [transforms.ToTensor(), transforms.Normalize(*norm)]\n",
    "\n",
    "    # Resize images if required\n",
    "    if resize_shape:\n",
    "        t.append(transforms.Resize(resize_shape))\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "\n",
    "def parse_annotations(path_dataset: str) -> list:\n",
    "    \"\"\"\n",
    "    Read dataset structure and extract path to images and annotations.\n",
    "    :param path_dataset: path to the dataset.\n",
    "    :return:\n",
    "        annotations: list of dictionaries, each with the path to the image, the bounding boxes, and the class labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Search labels on each sequence\n",
    "    annotations = []\n",
    "    for sequence in sorted(os.listdir(path_dataset)):\n",
    "        if os.path.isdir(os.path.join(path_dataset, sequence)):\n",
    "            path_sequence = os.path.join(path_dataset, 'labels')\n",
    "            # Search labels on each frame\n",
    "            for frame in sorted(os.listdir(path_sequence)):\n",
    "                if frame.endswith(\".txt\"):\n",
    "                    path_frame_labels = os.path.join(path_sequence, frame)\n",
    "                    # Load labels\n",
    "                    image_name, boxes, labels = read_content(path_frame_labels)\n",
    "\n",
    "                    # Get path to the image\n",
    "                    path_image = os.path.join(path_dataset, 'images', image_name)\n",
    "    \n",
    "                    # Save the path to the image, the boxes, and the labels (class of object) in a dictionary\n",
    "                    annotations.append({\n",
    "                        'path_image': path_image,\n",
    "                        'boxes': np.array(boxes),\n",
    "                        'labels': np.array(labels)\n",
    "                    })\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def read_content(txt_file: str) -> Tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Read annotation txt file.\n",
    "    :param txt_file: path to txt file.\n",
    "    :return:\n",
    "        image_name: string with the image filename.\n",
    "        list_with_all_boxes: list of bounding boxes.\n",
    "    \"\"\"\n",
    "    image_name = os.path.basename(txt_file).split('.')[0] + \".jpg\"\n",
    "\n",
    "    directorio = txt_file.rsplit('/', 2)[0]\n",
    "    \n",
    "    image_path = os.path.join(directorio, 'images', image_name)\n",
    "\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"La imagen {image_path} no existe.\")\n",
    "\n",
    "    with Image.open(image_path) as img:\n",
    "        image_width, image_height = img.size\n",
    "    \n",
    "    list_with_all_boxes = []\n",
    "    list_with_all_labels = []\n",
    "\n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            bbox_data = line.strip().split()\n",
    "            class_id = int(bbox_data[0])\n",
    "            x_center_norm = float(bbox_data[1])\n",
    "            y_center_norm = float(bbox_data[2])\n",
    "            width_norm = float(bbox_data[3])\n",
    "            height_norm = float(bbox_data[4])\n",
    "\n",
    "            x_center = x_center_norm * image_width\n",
    "            y_center = y_center_norm * image_height\n",
    "            width = width_norm * image_width\n",
    "            height = height_norm * image_height\n",
    "\n",
    "            xmin = int(x_center - (width / 2))\n",
    "            ymin = int(y_center - (height / 2))\n",
    "            xmax = int(x_center + (width / 2))\n",
    "            ymax = int(y_center + (height / 2))\n",
    "\n",
    "            list_with_single_boxes = [xmin, ymin, xmax, ymax]\n",
    "            list_with_all_boxes.append(list_with_single_boxes)\n",
    "            list_with_all_labels.append(class_id)\n",
    "\n",
    "    return image_name, list_with_all_boxes, list_with_all_labels\n",
    "\n",
    "\n",
    "def collate_fn(batch: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Avoids stacking images and annotations from dataloader as a Torch tensor, and stacks them as tuples.\n",
    "    :param batch: images and annotations loaded from dataset.\n",
    "    :return:\n",
    "        batch: images and annotations stacked as tuples.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "137dd09f-e4c5-431c-8a0f-9f89ffbcdc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "path_dataset_test = '/Test'                   \n",
    "\n",
    "# Model parameters\n",
    "model_name = 'retinanet_v2'                                               # Torchvision model\n",
    "model_path = f'/models/{model_name}/model_Adadelta_0001_24_v1.pt' # Path to save trained model\n",
    "num_classes = 2                                                             \n",
    "resize_shape = (1280, 720)                                                         # Resize images for faster performance. None to avoid resizing\n",
    "pretrained = True                                                           # Use weights pre-trained on COCO dataset\n",
    "\n",
    "# Train parameters\n",
    "batch_size = 4                                                              # Batch size\n",
    "use_gpu = True                                                              # Use GPU (True) or CPU (False)\n",
    "\n",
    "# Other parameters (do not change)\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d84594-353f-4f8a-8f75-490918ee840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, _, _, device = get_model(\n",
    "    model_name, model_path, num_classes, None, pretrained, use_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba5294f8-55da-4fda-abd2-938f64804b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = CustomDataset(path_dataset_test, resize_shape, transform=get_transform(\n",
    "    (get_inner_model(model).transform.image_mean, get_inner_model(model).transform.image_std), resize_shape\n",
    "))\n",
    "\n",
    "# Initialize your dataloader\n",
    "test_loader = DataLoader(\n",
    "    data_test, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a2530-5349-4ae1-9244-0bea9866e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model in eval mode (no gradients)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate (gradients are not necessary)\n",
    "predictions_list, targets_list = [], []\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Evaluate\n",
    "    print('Test:')\n",
    "    for images, targets in tqdm(test_loader):\n",
    "\n",
    "        # Move data to device\n",
    "        images = torch.stack(images, dim=0).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in targets[t].items()} for t in range(len(images))]\n",
    "        targets_list.extend(targets)\n",
    "\n",
    "        # Predict and get loss value\n",
    "        predictions = model(images)\n",
    "        predictions_list.extend(predictions)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = MeanAveragePrecision()(predictions_list, targets_list)\n",
    "    print('\\nmAP: {}'.format(metrics['map']))\n",
    "    print('mAP@0.50: {}'.format(metrics['map_50']))\n",
    "    print('mAP@0.75: {}'.format(metrics['map_75']))\n",
    "    print('mAP@Small: {}'.format(metrics['map_small']))\n",
    "    print('mAP@Medium: {}'.format(metrics['map_medium']))\n",
    "    print('mAP@Large: {}'.format(metrics['map_large']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
